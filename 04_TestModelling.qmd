---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}
# Required Libraries
library(mlr3verse)
library(mlr3spatial)
library(mlr3spatiotempcv)
library(tidyverse)
library(sf)
library(terra, exclude = "resample")
library(janitor)
library(future)
```

```{r include=FALSE}
# Set workspace
workspace_dir <- './'
source("mlr3_predict_spatial_prob.R")

# Set input/output file paths
raw_dir <- file.path(workspace_dir, "01_raw")
clim_dir <- file.path(workspace_dir, "05_climate_tif")
output_dir <- file.path(workspace_dir, "07_terrain_tif")
tif_models_dir <- file.path(workspace_dir, "09_models_tif")
ndvi_path <- list.files(raw_dir, pattern = "new_", full.names = TRUE)
terrain_vars <- list.files(output_dir, pattern = ".tif$", full.names = TRUE) 
clim_vars <- list.files(clim_dir, pattern = ".tif$", full.names = TRUE)
# dem_path <- file.path(output_dir, "dem.tif")
# slope_path <- file.path(output_dir, "slope.tif")
# aspect_path <- file.path(output_dir, "aspect.tif")
# long_curv_path <- file.path(output_dir, "long_curvature.tif")
# plan_curv_path <- file.path(output_dir, "plan_curvature.tif")
# tpi_path <- file.path(output_dir, "tpi.tif")
# tri_path <- file.path(output_dir, "tri.tif")
survey_path <- file.path(workspace_dir, "Survey_new.csv")
tif_filename = file.path(tif_models_dir, "output_raster.tif")
shp_filename = file.path(tif_models_dir, "output_raster.shp")
infestation_plot_path <- file.path(tif_models_dir, "infestation_level.png")

```

```{r}
# Load NDVI and Field Data
covariates <- rast(c(
  ndvi_path, terrain_vars, clim_vars)) %>% 
  clean_names()

field_points <- read.csv(survey_path, stringsAsFactors = FALSE) %>%
  clean_names() %>%
  rename(cover = x_cover) %>%
  mutate(cover = gsub("<|>", "", cover)) %>% 
  mutate(cover = gsub("\\s*\\([^\\)]+\\)","", cover)) %>%
  separate_wider_delim(cover, "-", names_sep = "-", too_few = "align_end") %>% 
  # mutate(`cover-2` = ifelse(is.na(`cover-1`), `cover-2`, `cover-1`)) %>%
  rename(cover = last_col()) %>% 
  dplyr::select(northing, westing, cover) %>% 
  mutate(cover = as.numeric(cover)) %>% 
  st_as_sf(coords = c("westing", "northing"), crs = 4326) %>% 
  st_transform(st_crs(covariates))
```

```{r}
# Convert field_points to SpatVector
field_points_vector <- terra::vect(field_points)

# Extract Raster Data for Points
raster_extract <- terra::extract(covariates, field_points_vector, ID = FALSE,
                                 fun = mean, na.rm = TRUE, bind = TRUE) %>%
  st_as_sf() %>%
  na.omit()

```

```{r}
set.seed(123)

task_raw <- as_task_regr_st(raster_extract, target = "cover")
```

Random Forest Regressor using recursive feature elimination, hyperparameter tuning, and spatial cross validation all together to determine the best feature and parameter set. Using from an example here: https://mlr3book.mlr-org.com/chapters/chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-featsel

```{r include=FALSE}

# # Define the model
# tune between 5 and total number of features
# Create the learner and feed it into a pipe for feature filtering. Note: this 
# is not the same as recursive feature elimination, but will only use the top
# number of features based on importance scores, so it is a close approximation
# of recursive feature elimination.
lrnr <- lrn("regr.ranger", importance = "impurity")
po_filter = po("filter", filter = flt("importance", learner = lrnr))
graph = as_learner(po_filter %>>% po("learner", lrnr))

# Explicitly define the machine learning parameters
params = ps(
  importance.filter.nfeat = p_fct(nlyr(covariates):5),
  regr.ranger.num.trees = p_fct(c(1, 250, 500, 1000, 1500, 2000)),
  regr.ranger.mtry.ratio = p_fct(c(0, 0.25, 0.5, 0.75, 1))
)
resamp <- rsmp("repeated_spcv_coords", folds = 10, repeats = 3)

# Begin modelling in parallel
plan("multisession", workers = min(c(availableCores(), resamp$iters)))

instance = suppressWarnings(mlr3tuning::tune(
  tnr("grid_search", resolution = 1), 
  task = task_raw,
  learner = graph,
  resampling = resamp, 
  terminator = trm("none"), 
  measures = msrs(c("oob_error", "regr.mse")),
  search_space = params,
  store_models = TRUE))

plan("sequential")

# Extract the model with the best scores. Best score here is defined as the 
# lowest score from a normalized dataset from either OOB errors or MSE.
arch <- as.data.table(instance$archive)
best_model_scores <- matrix(c(
  which.min(scale(arch$regr.mse)), min(scale(arch$regr.mse)), 
  which.min(scale(arch$oob_error)), min(scale(arch$oob_error))), ncol = 2)
best_model_rowid <- best_model_scores[1, which.min(best_model_scores[2, ])]
model_extract <- instance$archive$learners(best_model_rowid)

# Apply best model parameters and create a trained model
model_feats <- model_extract[[1]]$model$importance$features
model_params <- model_extract[[1]]$param_set$values
best_task <- task_raw$clone()
best_task$select(model_feats)
best_learner <- lrnr$clone()
best_learner$param_set$values = list(
  num.trees = model_params$regr.ranger.num.trees,
  mtry.ratio = model_params$regr.ranger.mtry.ratio,
  importance = model_params$regr.ranger.importance,
  num.threads = availableCores()
)
best_learner$train(best_task)

```

Create the map prediction

```{r}

if(file.exists(tif_filename)) unlink(tif_filename)
map_prediction <- predict_spatial(covariates, best_learner, filename = tif_filename)

## from-to-becomes
# classify the values into multiple groups 
m <- data.frame(
  from = c(0, 1, 5, 15, 30, 45),
  to = c(1, 5, 15, 30, 45, 100),
  id = c(0, 1, 2, 3, 4, 5),
  cover = c("Free (0%)",
            "Trace (1-5%)",
            "Light Infestation (5-15%)",
            "Mild Infestation (15-30%)",
            "Moderate Infestation (30-45%)",
            "Cheatgrass Dominated (45-100%)"),
  colors = c("white", "#00BFFF", "#ADFF2F", "#FFD700", "#FF8C00", "#FF0000"))
rclmat <- as.matrix(m[, 1:3])
rcl <- classify(map_prediction, rclmat, include.lowest = TRUE)
coltab(rcl) <- m$colors
levels(rcl) <- m[, 3:4]

rcl <- writeRaster(rcl, file.path(tif_models_dir, "infestation_level.tif"), 
                   overwrite = TRUE)

```

Random forest classifier model (re-inserted from previously)

The code below has been copy/pasted from the predict_spatial() function, and has been customized to provide an output of the probability of cheatgrass being present rather than its classified output. It currently works well with untuned models, and autotuning the models will be critical for getting this working for the rest of it. Works with both random forest (preferred for spatial data) and SVM's.

```{r results="hide"}

field_points_classif <- field_points |> 
  mutate(presence = as.factor(cover > 0)) |> 
  dplyr::select(-cover)

# Convert field_points to SpatVector
field_points_classif_vector <- terra::vect(field_points_classif)

# Extract Raster Data for Points
raster_extract_classif <- terra::extract(covariates, field_points_classif_vector, ID = FALSE,
                                 fun = mean, na.rm = TRUE, bind = TRUE) %>%
  st_as_sf() %>%
  na.omit()

# Define the model: task = data, learner = model used
task_raw_classif <- as_task_classif_st(raster_extract_classif, target = "presence", positive = "TRUE")
lrnr_classif <- lrn("classif.ranger", predict_type = "prob", importance = "impurity")

# Create a feature filtering pipeop learner, allowing it to be used in a tuned model
po_filter_classif = po("filter", filter = flt("importance", learner = lrnr_classif))
graph_classif = as_learner(po_filter_classif %>>% po("learner", lrnr_classif))

# Explicitly define the machine learning parameters
params_classif = ps(
  importance.filter.nfeat = p_fct(nlyr(covariates):5),
  classif.ranger.num.trees = p_fct(c(1, 250, 500, 1000, 1500, 2000)),
  classif.ranger.mtry.ratio = p_fct(c(0, 0.25, 0.5, 0.75, 1))
)
resamp <- rsmp("repeated_spcv_coords", folds = 10, repeats = 3)

# Begin modelling in parallel
plan("multisession", workers = min(c(availableCores(), resamp$iters)))

# Tuning will happen across the number of features sorted by their feature 
# importance, a defined set of numbers of trees, and a defined set of mtry 
# ratios. Each individual model will be repeated in a 10-fold spatial cross=
# validation repeated 3 times. Each model is stored in memory, and with tens
# of thousands of stored models this will explode your RAM usage.
instance_classif = suppressWarnings(mlr3tuning::tune(
  tnr("grid_search", resolution = 1), 
  task = task_raw_classif,
  learner = graph_classif,
  resampling = resamp, 
  terminator = trm("none"), 
  measures = msrs(c("oob_error", "classif.ce")),
  search_space = params_classif,
  store_models = TRUE))

plan("sequential")

# Extract the model with the best scores. Best score here is defined as the 
# lowest score from a normalized dataset from either OOB errors or MSE.
arch_classif <- as.data.table(instance_classif$archive)
best_classif_model_scores <- matrix(c(
  which.min(scale(arch_classif$classif.ce)), min(scale(arch_classif$classif.ce)), 
  which.min(scale(arch_classif$oob_error)), min(scale(arch_classif$oob_error))), ncol = 2)
best_classif_model_rowid <- best_classif_model_scores[1, which.min(best_classif_model_scores[2, ])]
model_extract_classif <- instance_classif$archive$learners(best_classif_model_rowid)

# Apply best model parameters and create a trained model
model_feats_classif <- model_extract_classif[[1]]$model$importance$features
model_params_classif <- model_extract_classif[[1]]$param_set$values
best_task_classif <- task_raw_classif$clone()
best_task_classif$select(model_feats_classif)
best_learner_classif <- lrnr_classif$clone()
best_learner_classif$param_set$values = list(
  num.trees = model_params_classif$classif.ranger.num.trees,
  mtry.ratio = model_params_classif$classif.ranger.mtry.ratio,
  importance = model_params_classif$classif.ranger.importance,
  num.threads = availableCores()
)
best_learner_classif$train(best_task_classif)

tif_filename_classif <- file.path(tif_models_dir, "output_prob.tif")
if(file.exists(tif_filename_classif)) unlink(tif_filename_classif)
map_prediction_classif <- predict_spatial_prob(covariates, best_learner_classif,
                                               filename = tif_filename_classif)

# Create classification bins for the probabilities. Not quite as useful as 
# above, but the format here makes it adjustable if needed.

## from-to-becomes
# classify the values into multiple groups 
m_classif <- data.frame(
  from = c(0, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9),
  to = c(0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
  id = c(0, 1, 2, 3, 4, 5, 6),
  probability = c("<25%",
           "25-50%",
           "50-60%",
           "60-70%",
           "70-80%",
           "80-90%",
           "90-100%"),
  colors = c("white", "#00BFFF", "#ADFF2F", "#FFD700", "#FF8C00", "red1", "darkred"))
rclmat_classif <- as.matrix(m_classif[, 1:3])
rcl_classif <- classify(map_prediction_classif$`TRUE`, rclmat_classif, include.lowest = TRUE)
coltab(rcl_classif) <- m_classif$colors
levels(rcl_classif) <- m_classif[, 3:4]

rcl_classif <- writeRaster(rcl_classif, file.path(tif_models_dir, "prob_class.tif"), 
                   overwrite = TRUE)

```
